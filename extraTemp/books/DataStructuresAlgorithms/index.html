<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"/><meta name="og:site_name" content="Cabulas"/><link rel="canonical" href="https://jjorgemoura.github.io/extraTemp/books/DataStructuresAlgorithms"/><meta name="twitter:url" content="https://jjorgemoura.github.io/extraTemp/books/DataStructuresAlgorithms"/><meta name="og:url" content="https://jjorgemoura.github.io/extraTemp/books/DataStructuresAlgorithms"/><title>A Common-Sense Guide to Data Structures and Algorithms | Cabulas</title><meta name="twitter:title" content="A Common-Sense Guide to Data Structures and Algorithms | Cabulas"/><meta name="og:title" content="A Common-Sense Guide to Data Structures and Algorithms | Cabulas"/><meta name="description" content="I'm an iOS Software Engineer"/><meta name="twitter:description" content="I'm an iOS Software Engineer"/><meta name="og:description" content="I'm an iOS Software Engineer"/><meta name="twitter:card" content="summary"/><link rel="stylesheet" href="/styles.css" type="text/css"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><link rel="shortcut icon" href="/images/favicon.png" type="image/png"/><link rel="alternate" href="/feed.rss" type="application/rss+xml" title="Subscribe to Cabulas"/></head><body><header><div class="wrapper"><a href="/" class="site-name">Cabulas</a><nav><ul><li><a href="/articles">Articles</a></li><li><a href="/tips">Tips</a></li><li><a href="/posts">My posts</a></li></ul></nav></div></header><div class="wrapper"><h1>A Common-Sense Guide to Data Structures and Algorithms</h1><h3>by: Jay Wengrow</h3><h2>Why Data Structures Matter</h2><p><strong>Data</strong> is a broad term that refer to all types of information.</p><p>Computer programs are all about receiving, manipulating and returning data.</p><p>The four basic operations used by most data structures are four:</p><ul><li>Read</li><li>Search</li><li>Insert</li><li>Delete</li></ul><p>One key aspect while using these data structures is how those data structures perform executing basic operations.</p><p>We measure how fast are the data structures. However, &gt; when we measure how fast an operation takes, we do not refer to how fast the operation takes in terms of pure time, but instead in how many steps it takes.</p><h3>The Array</h3><p>Is a list of data elements.</p><ul><li>Read -&gt; O(1)</li><li>Search -&gt; O(N)</li><li>Insert -&gt; O(N+1) (O(1) if at end)</li><li>Delete -&gt; O(N+1) (O(1) if at end)</li></ul><h3>The Set (particular, an "Array-based" Set)</h3><p>This version of the <strong>Set</strong> is a basically an <strong>array</strong> without duplicated elements. So, it is linked list! (to confirm, means we can access by index and the order is kept)</p><ul><li>Read -&gt; O(1)</li><li>Search -&gt; O(N)</li><li>Insert -&gt; O(N+1) if at the end, O(2N+1) if not</li><li>Delete -&gt; O(N)</li></ul><h3>The Ordered-Array</h3><p>Same structure as the <strong>Array</strong> however the values are always <em>in order</em>. During each insertion, the new value is inserted in their "order".</p><ul><li>Read -&gt;</li><li>Search -&gt; O(N) for linear search, O(logN) for binary search</li><li>Insert -&gt;</li><li>Delete -&gt;</li></ul><h2>Why Algorithms Matter</h2><blockquote><p>An algorithm is simply a particular process for solving a problem.</p></blockquote><p>When applied to computing, an algorithm refers to a process for going about a particular operation.</p><p>The basic operations to be analysed here are:</p><ul><li>Search</li><li>Sorting</li></ul><p>Basically we are confined to these operations because the other operations rely on mostly on search.</p><p>We can see it is possible to go about an operation in more than one way. Thus, we can say there are multiple algorithms that can achieve a particular operation.</p><h3>Linear Search</h3><p>Search is O(N)</p><h3>Binary Search</h3><p>Search is O(log N) However, only works with sorted arrays.</p><p>https://en.wikipedia.org/wiki/Binary<em>search</em>algorithm</p><h2>Big O</h2><p>Big O has its origin in maths.</p><p>There is the concept of Big O for the best-scenario, average-scenario and the worse-scenario. In computer science, the Big O notation usually addresses the worse case scenario.</p><p>Instead of focusing on units of time, Big O achieves consistency by focusing only on the number of steps that an algorithm takes.</p><p><strong>O (1)</strong> - Big Oh of 1, or Oh of 1. Only takes one step.</p><p>In some nomenclature, in order to simplify, and because by the end the impact is basically null, even if the algorithm takes 2 or 3 or 20 steps, we classify that algorithm's performance as O(1).</p><p>Over time, O(1) is also known as <strong><em>constant time</em></strong>.</p><p><strong>O (log N)</strong> - Big Oh of log N, or Oh of log N. Increases one step each time the data is doubled.</p><p>Over time, O(log N) is also known as <strong><em>logarithmic time</em></strong>.</p><p><strong>O (N)</strong> - Big Oh of N, or Oh of N. Takes N steps, the same as the size of the collection.</p><p>Over time, O(N) is also known as <strong><em>linear time</em></strong>.</p><p>So, more than indicate the number of steps an algorithm takes, the important characteristic is the growth rate over time. Thus:</p><ul><li>Constant time</li><li>Logarithmic time</li><li>Linear time</li><li>quadratic time</li><li>XXX time</li></ul><h2>Speeding Up Code with Big O</h2><p>Sorting algorithms are an important category that uses Big O to assess their efficiency.</p><h4>Bubble Sort</h4><p>In terms of efficiency, the bubble sort algorithm performs two kinds of steps:</p><ul><li>comparisons</li><li>swaps</li></ul><p>For N = 5, the bubble sort algorithm performs 10 comparisons and (in worse case) 10 swaps. So, total of 20 steps. The Bubble sort algorithm has a efficiency of O (N^2).</p><p><strong>O (N^2)</strong> - Big Oh of N square, or Oh of N square. Takes N^2 steps.</p><p>Over time, O(N^2) is also known as <strong><em>quadratic time</em></strong>.</p><p><strong><em>NOTE:</em></strong> - Big O measures performance based on the number of <strong>steps</strong> required to perform. In order to analyse the algorithm, we need to identify what qualifies as a step.</p><h2>Optimising Code with and without Big O</h2><p>How to discern between two algorithms that seems to have the same efficiency in terms of Big O but actually have different performances.</p><h4>Selection Sort</h4><p>Even if the Selection Sort seems to perform at O(N^2 / 2), or more specifically, needs slightly more than half of the Bubble Sort step, there is one Big O <strong>rule</strong> that says <strong><em>Big O Notation ignores constants</em></strong>.</p><p>So, even if an algorithm is 100 times faster than other, e.g. O(100N) vs O(N), under Big O notation, the performance of both are the same (O(N)) under Big O Notation.</p><p>The decision to ignore constants is because, when we compare how the number of steps grow over time, what is really important is the time factor, if it is linear of quadratic. Even if for small number of elements a N^2 could be faster than 100N, after some time the N^2 will always take more steps to finish.</p><p>We can assume the Big O notation is good to classify contrasting algorithms that fall under different classifications of Big O. If under the same classification, further analysis is required to identify which algorithm is faster.</p><h2>Optimising for Optimistic Scenarios</h2><h4>Insertion Sort</h4><p>This algorithm performs several type of steps: comparisons, insertions, shifts and removals.</p><p>For N = 5, in the worst case, the insert sort algorithm performs 10 comparisons and 10 swaps. It performs always N-1 removals and insertions. So, in total, it has N^2 + 2N -2 steps. By the rule of removing constants, we can say it has N^2 + N steps.</p><p>However, there is another Big O <strong>rule</strong>. It says that <strong><em>Big O Notation only takes into account the highest order of N.</em></strong>.</p><p>So, the Insertion sort algorithm has a efficiency of O (N^2) for worst case scenario.</p><p><strong><em>Note</em></strong> Best and worst-case scenarios only rarely happerns. The most cases are the average scenarios.</p><h2>Blazing Fast Lookup with Hash Tables</h2><p>Hash Tables are also know as <strong><em>Dictionaries</em></strong> (or also "maps", "hash maps"). Hast Tables have a super power, theirs "<em>fast reading</em>".</p><p>The Hash Tables are difined by the pair <strong><em>key/value</em></strong>.</p><p>Looking up a value has a efficiency of O(1).</p><p>The name hash come from the <strong><em>hashing function</em></strong> used to make sure the keys are unique. <strong>Hash Functions</strong> need to respect one criteria, they must convert the <strong><em>same string into the same number every single time it's applied</em></strong>.</p><p>So the lookup only takes 1 step because the lookup process is passing the "search" key into the hash function and get the unique result that is, basically, the index in the physicall structure of the hash.</p><p>In case the hash function produces some collisions, the way the structure hasndles that is not storing the value in the structure directly but instead in a array (one entry by each key/value) with subarrays (subarray with two positions, first the key, second the value.). This extra work means the lookup can take more than just 1 step. So, using a bad "hash function" could make the hash table lookup takes O(N) steps.</p><p>The Hashtable effeciency of the hash tables depends on:</p><ul><li>how much data is stored</li><li>how many cells are available</li><li>which hash function we're using</li></ul><p>A hash table is a theorectic structure. Swift's dictionary is one implementation, with their own hash function and storage strategy.</p><p><strong><em>Note:</em></strong> As a rule of thumb, "<strong><em>for evert seven data elements stored in a hash table, it should have ten cells</em></strong>". The ratio of data to cells is called the "<strong><em>load factor</em></strong>". So, the ideal factor should be 0.7.</p><h2>Crafting Elegant code with Stacks and Queues</h2><p>Stacks and Queues are arrays with some rules/restrictions. Stacks and Queues are elegant tools for handling temporary data.</p><h3>Stacks</h3><p>Is basically an array with 3 rules:</p><ul><li>Data can only be inserted at the end of a stack</li><li>Data can only be read from the end of a stack</li><li>Data can only be removed from the end of a stack</li></ul><p>The end of the stack is called the "top". Removing an element is calling "popping".</p><p>A Stack can be described as a <strong>LIFO</strong>.</p><p>Stacks are indeal for processing any data that should be handled in reverse order to how it was received.</p><h3>Queues</h3><p>Is basically also an array with 3 rules:</p><ul><li>Data can only be inserted at the end of a stack</li><li>Data can only be read from the front of a stack</li><li>Data can only be removed from the front of a stack</li></ul><p>A Stack can be described as a <strong>FIFO</strong>.</p><h2>Recursively Recurse with Recursion</h2><p><strong><em>Recursion</em></strong> - When a function calls itself. Recursion can be used instead of a for loop.</p><p>The case in which the method will not recurse is known as the <strong><em>base case</em></strong>.</p><p>In order develop and test a recursive method we can/should start from the base case and increment the case. Since we already have the result of the previous case, then will be super easy and fast to test each case. Starting from the base case and building up is a great way to reason about recursive code.</p><p>The computer uses a "stack" to keep track of which functions it's in the middle of calling. This stack is known as the "<strong><em>call stack</em></strong>".</p><p>This is also why when a recursive function doesn't have a base case, the computer keeps on pushing the same method into the stack until the computer's memory is totally full, causing an error called "<strong><em>stack overflow!</em></strong>".</p><h2>Recursive Algorithms for Speed</h2><p>Most languauges give us already a <strong><em>sort</em></strong> algorithm by default. This algorithm is usually the <strong><em>Quicksort</em></strong>. Quicksort relies on recursion. It is particularlly fast for for average scenarios, even though is quite similar with the Insertion and Selection sort in the worst scenarios.</p><p>Quicksort relies on a concept called <strong><em>partitioning</em></strong>.</p><h4>Partitioning</h4><p>The quicksort has 2 types of steps:</p><ul><li>comparisons</li><li>swaps</li></ul><p>Each partition compares N times. Each partition has at least 1 swap. The most swaps a partication can have is N/2.</p><p>So, for average scenarios, it would be N/4 swaps per partition. Adding up, there are 5N/4 steps (1.25 steps). By Big O notation rules, we can constants. So, there a just N steps (O(N) time) per partition. So, how about the number of partitions. Remember we don't perfom any step on partitions with 1 element. So, for N elements we will have N partitions. However, some are "1 element" partitions. Basically there nothing more to calculate. Each partition is O(N).</p><p>We can approximate the effeciency to be O(N logN). The logarithm concept already come from the binary search. Basically we split the elements in half.</p><p><strong>O (N logN)</strong> - Big Oh of N logN, or Oh of N logN.</p><p>However, the quicksort is a O(N^2) for the worst case.</p><p><strong>Note</strong>: For the best scenario, the quicksort also performs as O(N logN). This is worse that others sort algorithms for the best scenario, example, the <em>Insertion Sort</em>.</p><h4>Quickselect</h4><p>Select sorted but without sorting. This is a beatifull algorithm that mixes Quicksort wth BinaryTrees to quickly identify the <strong><em>nth</em></strong> element of a list.</p><h2>Node-Based Data Structures</h2><p>Structures build upon the <strong><em>node</em></strong> concept.</p><h4>Linked-List</h4><p>It is the simplest node-based data structure. It's quite similar with arrays but with their own trade-offs on efficiency.</p><p>In computer's memory, an Array is a <strong>contiguous</strong> group of <strong>cells</strong>. This is why computer can locate the cell in one step. He knows where the array starts (in terms of memory address) so locating the th index is just a matter of sum that to the initial address and gets the index address.</p><p>Linked-List, on other hand, doesn't rely on contiguous memory cells. These cells that are not adjecent to each other are known as <strong><em>nodes</em></strong>.</p><p>Basically, each node stores not only their data but also the memory addess of the next node. This memory address pointer is known as a <strong><em>link</em></strong>. Each node takes more memory (2 cells) than a simple Array. So, linked-list takes more space but is has the advantage of not needing to find a contiguous group of cells to store the data.</p><ul><li>Read -&gt; O(N)</li><li>Search -&gt; O(N)</li><li>Insert -&gt; O(1) if at the beggining, O(N+1) if end</li><li>Delete -&gt; O(N) (O(1) if at the beggining, O(N+1) if end)</li></ul><p>However, there are big advantages over linked-lists. We can transverse the array and "delete" at same time, with that delete being O(1), not O(N).</p><h4>Doubly Linked-List</h4><p>Same as Linked-list but know also keeping the address memory of the previous node. This has a big advantage. Since the <strong><em>Doubly linked list</em></strong> always knows where both its first and last nodes are, we can access each of them in a single step O(1). Thus, now insert and delete are O(1) for operations at the front and end of the list.</p><h2>Speeding Up All the Things with Binary Trees</h2><h4>Binary Tree</h4><p>Basically is a HashTable that keeps order, and that is achieved using a node-based structure.</p><p>A <strong><em>Tree</em></strong> differs from the <strong><em>Linked-List</em></strong> as each node has multiple sub-nodes. A <strong><em>Binary-Tree</em></strong> is a particular case, it has 2 sub-nodes.</p><p>Trees:</p><ul><li>root: top most node</li><li>parent/child relations</li><li>root is at first level. Their childs are at second level.</li></ul><p>Binary trees:</p><ul><li>each node has either zero, one or two cildren</li><li>if a node has 2 children, is must have one child that has a lesser value than the parent, and one child that has a greater value than the parent.</li></ul><ul><li>Read -&gt; O()</li><li>Search -&gt; O(log N) -&gt; However, this is only valid for well balanced trees.</li><li>Insert -&gt; O(log N + 1) -&gt; Same as search the position + 1 to insert</li><li>Delete -&gt; O(log N) -&gt; Actually is O(logN + 1) or O(logN + 2) or even O(2 logN + 1), I think.</li></ul><p>Not that imbalanced trees are bad. Search is O(N) if the tree is imbalanced.</p><p>Delete is the most tricky situations. If delete a leaf, is straighforward. However, if we are deleting a node, then we need to move the childs up (promote one of the childs to be a node). Delete has 4 rules depending on which node is (if has childs).</p><h3>Traversing a Tree</h3><p>There are 3 ways:</p><ul><li>Pre</li><li>Inorder</li><li>Pos</li></ul><h2>Connecting Everything with Graphs</h2><p>A graph were a node is called <strong><em>vertex</em></strong> and a line/relation is called a <strong><em>edge</em></strong>.</p><p>Vertices connected are called adjacents.</p><p>The edges could be <strong><em>directional</em></strong>, making the graphs:</p><ul><li><em>directed graph</em></li><li><em>non-directed graph</em></li></ul><p>There are two ways to traverse a graph:</p><ul><li>breadth-first search</li><li>depth-first search</li></ul><p>The <strong><em>breadth-first search</em></strong> uses a queue.</p><p><strong>Dijkstra's algorithm</strong></p><h2>Dealing with Space Constraints</h2><p>As important as the <strong><em>time complexity</em></strong> is also the <strong><em>space complexity</em></strong>.</p><p>Space coplexity is particularily important when programming for small hardware devices.</p><p>We can use Big (O) notation also for space complexity analysis, however steps are now data in memory.</p><p>By space complexity we consider the data/memory necessary by the algorithm itself, not consider the since of the input.</p><p>If we create an array to return, typically will be O(N). If we don't need to create anything, we say it's a O(1), the space used is constant no matter the size of N.</p><h2>Resources</h2><p><a href="http://bigoref.com">http://bigoref.com</a></p><p><a href="http://bigocheatsheet.com">http://bigocheatsheet.com</a></p></div><footer><p>Generated using <a href="https://github.com/johnsundell/publish">Publish</a></p><p><a href="/feed.rss">RSS feed</a></p></footer></body></html>